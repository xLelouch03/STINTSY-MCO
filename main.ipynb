{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STINTSY MCO - Pumpkin Seeds Dataset\n",
    "**STINTSY S12** - BANTOLINO, Jana Marie S., DIMALANTA, Jason Erwin Clyde V., JAWALI, Armina R., REJANO, Hans Martin F."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "In the agricultural industry, the ability to classify pumpkin seeds by type is essential for efficient seed sorting, quality control, and market distribution. Accurate seed type identification allows farmers and distributors to ensure that the seeds meet specific requirements for growth, yield, and commercial viability.\n",
    "\n",
    "In this project, we aim to develop a machine learning model to classify pumpkin seeds into two primary types: Cercevelik and Urgup Sivrisi. Using a dataset that includes various morphological features of the seeds—such as area, perimeter, axis lengths, compactness, and eccentricity—we will explore, preprocess, and analyze the data to build a robust classification model. This model should be able to accurately predict the type of a pumpkin seed based on its physical characteristics, providing a valuable tool for farmers and agricultural businesses to streamline the sorting and distribution process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pumpkin Seeds Dataset provides detailed morphological and nutritional data on two Turkish pumpkin seed varieties, Urgup Sivrisi and Cercevelik (Topak), both belonging to the Cucurbitaceae family. Derived from Cucurbita pepo L and occasionally Cucurbita moschata Duchesne, these seeds are nutritionally valuable, containing ~37% carbohydrates, 35–40% fats and proteins, and essential minerals like calcium, magnesium, and zinc. \n",
    "\n",
    "The dataset includes features such as geometric properties (area, perimeter, major and minor axis lengths), shape descriptors (eccentricity, roundness, aspect ratio, compactness), and a classification label identifying the seed type. More information about the different features can be found on the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute             | Description                                                                                           | Data Type |\n",
    "|-----------------------|-------------------------------------------------------------------------------------------------------|-----------|\n",
    "| **Area**              | Number of pixels within the borders of a pumpkin seed                                                 | int64     |\n",
    "| **Perimeter**         | Circumference in pixels of a pumpkin seed                                                             | float64   |\n",
    "| **Major_Axis_Length** | Large axis distance of a pumpkin seed                                                                 | float64   |\n",
    "| **Minor_Axis_Length** | Small axis distance of a pumpkin seed                                                                 | float64   |\n",
    "| **Convex_Area**       | Number of pixels of the smallest convex shell at the region formed by the pumpkin seed                | int64     |\n",
    "| **Equiv_Diameter**    | Computed as $\\sqrt{\\frac{4a}{\\pi}}$, where $a$ is the area of the pumpkin seed                                 | float64   |\n",
    "| **Eccentricity**      | Eccentricity of a pumpkin seed                                                                        | float64   |\n",
    "| **Solidity**          | Convex condition of the pumpkin seeds                                                                 | float64   |\n",
    "| **Extent**            | Ratio of a pumpkin seed area to the bounding box pixels                                               | float64   |\n",
    "| **Roundness**         | Ovality of pumpkin seeds without considering the distortion of the edges                              | float64   |\n",
    "| **Aspect_Ratio**      | Aspect ratio of the pumpkin seeds                                                                     | float64   |\n",
    "| **Compactness**       | Proportion of the area of the pumpkin seed relative to the area of the circle with the same circumference | float64 |\n",
    "| **Class**             | Seed type, either Cercevelik or Urgup Sivrisi                                                         | object    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: List of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from scipy.stats import pointbiserialr\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Data preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Major_Axis_Length</th>\n",
       "      <th>Minor_Axis_Length</th>\n",
       "      <th>Convex_Area</th>\n",
       "      <th>Equiv_Diameter</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Roundness</th>\n",
       "      <th>Aspect_Ration</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56276</td>\n",
       "      <td>888.242</td>\n",
       "      <td>326.1485</td>\n",
       "      <td>220.2388</td>\n",
       "      <td>56831</td>\n",
       "      <td>267.6805</td>\n",
       "      <td>0.7376</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.7453</td>\n",
       "      <td>0.8963</td>\n",
       "      <td>1.4809</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>erevelik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76631</td>\n",
       "      <td>1068.146</td>\n",
       "      <td>417.1932</td>\n",
       "      <td>234.2289</td>\n",
       "      <td>77280</td>\n",
       "      <td>312.3614</td>\n",
       "      <td>0.8275</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.7151</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>1.7811</td>\n",
       "      <td>0.7487</td>\n",
       "      <td>erevelik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71623</td>\n",
       "      <td>1082.987</td>\n",
       "      <td>435.8328</td>\n",
       "      <td>211.0457</td>\n",
       "      <td>72663</td>\n",
       "      <td>301.9822</td>\n",
       "      <td>0.8749</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>0.7674</td>\n",
       "      <td>2.0651</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>erevelik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66458</td>\n",
       "      <td>992.051</td>\n",
       "      <td>381.5638</td>\n",
       "      <td>222.5322</td>\n",
       "      <td>67118</td>\n",
       "      <td>290.8899</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.7396</td>\n",
       "      <td>0.8486</td>\n",
       "      <td>1.7146</td>\n",
       "      <td>0.7624</td>\n",
       "      <td>erevelik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66107</td>\n",
       "      <td>998.146</td>\n",
       "      <td>383.8883</td>\n",
       "      <td>220.4545</td>\n",
       "      <td>67117</td>\n",
       "      <td>290.1207</td>\n",
       "      <td>0.8187</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.6752</td>\n",
       "      <td>0.8338</td>\n",
       "      <td>1.7413</td>\n",
       "      <td>0.7557</td>\n",
       "      <td>erevelik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>79637</td>\n",
       "      <td>1224.710</td>\n",
       "      <td>533.1513</td>\n",
       "      <td>190.4367</td>\n",
       "      <td>80381</td>\n",
       "      <td>318.4289</td>\n",
       "      <td>0.9340</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.4888</td>\n",
       "      <td>0.6672</td>\n",
       "      <td>2.7996</td>\n",
       "      <td>0.5973</td>\n",
       "      <td>rgp Sivrisi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>69647</td>\n",
       "      <td>1084.318</td>\n",
       "      <td>462.9416</td>\n",
       "      <td>191.8210</td>\n",
       "      <td>70216</td>\n",
       "      <td>297.7874</td>\n",
       "      <td>0.9101</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>0.6002</td>\n",
       "      <td>0.7444</td>\n",
       "      <td>2.4134</td>\n",
       "      <td>0.6433</td>\n",
       "      <td>rgp Sivrisi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>87994</td>\n",
       "      <td>1210.314</td>\n",
       "      <td>507.2200</td>\n",
       "      <td>222.1872</td>\n",
       "      <td>88702</td>\n",
       "      <td>334.7199</td>\n",
       "      <td>0.8990</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>0.7643</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>2.2828</td>\n",
       "      <td>0.6599</td>\n",
       "      <td>rgp Sivrisi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>80011</td>\n",
       "      <td>1182.947</td>\n",
       "      <td>501.9065</td>\n",
       "      <td>204.7531</td>\n",
       "      <td>80902</td>\n",
       "      <td>319.1758</td>\n",
       "      <td>0.9130</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.7374</td>\n",
       "      <td>0.7185</td>\n",
       "      <td>2.4513</td>\n",
       "      <td>0.6359</td>\n",
       "      <td>rgp Sivrisi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>84934</td>\n",
       "      <td>1159.933</td>\n",
       "      <td>462.8951</td>\n",
       "      <td>234.5597</td>\n",
       "      <td>85781</td>\n",
       "      <td>328.8485</td>\n",
       "      <td>0.8621</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>0.7933</td>\n",
       "      <td>1.9735</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>rgp Sivrisi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area  Perimeter  Major_Axis_Length  Minor_Axis_Length  Convex_Area  \\\n",
       "0     56276    888.242           326.1485           220.2388        56831   \n",
       "1     76631   1068.146           417.1932           234.2289        77280   \n",
       "2     71623   1082.987           435.8328           211.0457        72663   \n",
       "3     66458    992.051           381.5638           222.5322        67118   \n",
       "4     66107    998.146           383.8883           220.4545        67117   \n",
       "...     ...        ...                ...                ...          ...   \n",
       "2495  79637   1224.710           533.1513           190.4367        80381   \n",
       "2496  69647   1084.318           462.9416           191.8210        70216   \n",
       "2497  87994   1210.314           507.2200           222.1872        88702   \n",
       "2498  80011   1182.947           501.9065           204.7531        80902   \n",
       "2499  84934   1159.933           462.8951           234.5597        85781   \n",
       "\n",
       "      Equiv_Diameter  Eccentricity  Solidity  Extent  Roundness  \\\n",
       "0           267.6805        0.7376    0.9902  0.7453     0.8963   \n",
       "1           312.3614        0.8275    0.9916  0.7151     0.8440   \n",
       "2           301.9822        0.8749    0.9857  0.7400     0.7674   \n",
       "3           290.8899        0.8123    0.9902  0.7396     0.8486   \n",
       "4           290.1207        0.8187    0.9850  0.6752     0.8338   \n",
       "...              ...           ...       ...     ...        ...   \n",
       "2495        318.4289        0.9340    0.9907  0.4888     0.6672   \n",
       "2496        297.7874        0.9101    0.9919  0.6002     0.7444   \n",
       "2497        334.7199        0.8990    0.9920  0.7643     0.7549   \n",
       "2498        319.1758        0.9130    0.9890  0.7374     0.7185   \n",
       "2499        328.8485        0.8621    0.9901  0.7360     0.7933   \n",
       "\n",
       "      Aspect_Ration  Compactness          Class  \n",
       "0            1.4809       0.8207     erevelik  \n",
       "1            1.7811       0.7487     erevelik  \n",
       "2            2.0651       0.6929     erevelik  \n",
       "3            1.7146       0.7624     erevelik  \n",
       "4            1.7413       0.7557     erevelik  \n",
       "...             ...          ...            ...  \n",
       "2495         2.7996       0.5973  rgp Sivrisi  \n",
       "2496         2.4134       0.6433  rgp Sivrisi  \n",
       "2497         2.2828       0.6599  rgp Sivrisi  \n",
       "2498         2.4513       0.6359  rgp Sivrisi  \n",
       "2499         1.9735       0.7104  rgp Sivrisi  \n",
       "\n",
       "[2500 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "data = pd.read_csv('dataset/pumpkin_seeds.csv', encoding='latin1')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output above, the dataset contains 13 columns, 12 of which are the features and the last column being the target label or class of the pumpkin seed. It also contains 2500 rows or data instances of pumpkin seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Special Characters on the Class column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data have shown that the class column contains special characters for each of its unique values, there is a need to remove this to make sure that it contains the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cercevelik' 'Urgup Sivrisi']\n"
     ]
    }
   ],
   "source": [
    "data['Class'] = data['Class'].replace({'\\x82er\\x8develik': 'Cercevelik'})\n",
    "data['Class'] = data['Class'].replace({'\\x86rg\\x9fp Sivrisi': 'Urgup Sivrisi'})\n",
    "\n",
    "print(data['Class'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for NaN / Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each row of the dataset does not have any null values, there is no need to handle any NaN / missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon looking at the unique values of the class label, the values are all characters and since the main task of this project is to perform classification of the pumpkin seeds, it is better if the values are converted to numerical representations through Label Encoding. Label Encoding is an approach used for binary classification where there are only two possible classes and it just assigns a value of 0 or 1 to either of the classes. This approach will reduce the complexity of representing labels since only 0 or 1 labels are used for training the machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Major_Axis_Length</th>\n",
       "      <th>Minor_Axis_Length</th>\n",
       "      <th>Convex_Area</th>\n",
       "      <th>Equiv_Diameter</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Roundness</th>\n",
       "      <th>Aspect_Ration</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56276</td>\n",
       "      <td>888.242</td>\n",
       "      <td>326.1485</td>\n",
       "      <td>220.2388</td>\n",
       "      <td>56831</td>\n",
       "      <td>267.6805</td>\n",
       "      <td>0.7376</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.7453</td>\n",
       "      <td>0.8963</td>\n",
       "      <td>1.4809</td>\n",
       "      <td>0.8207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76631</td>\n",
       "      <td>1068.146</td>\n",
       "      <td>417.1932</td>\n",
       "      <td>234.2289</td>\n",
       "      <td>77280</td>\n",
       "      <td>312.3614</td>\n",
       "      <td>0.8275</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>0.7151</td>\n",
       "      <td>0.8440</td>\n",
       "      <td>1.7811</td>\n",
       "      <td>0.7487</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71623</td>\n",
       "      <td>1082.987</td>\n",
       "      <td>435.8328</td>\n",
       "      <td>211.0457</td>\n",
       "      <td>72663</td>\n",
       "      <td>301.9822</td>\n",
       "      <td>0.8749</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>0.7674</td>\n",
       "      <td>2.0651</td>\n",
       "      <td>0.6929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66458</td>\n",
       "      <td>992.051</td>\n",
       "      <td>381.5638</td>\n",
       "      <td>222.5322</td>\n",
       "      <td>67118</td>\n",
       "      <td>290.8899</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.7396</td>\n",
       "      <td>0.8486</td>\n",
       "      <td>1.7146</td>\n",
       "      <td>0.7624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66107</td>\n",
       "      <td>998.146</td>\n",
       "      <td>383.8883</td>\n",
       "      <td>220.4545</td>\n",
       "      <td>67117</td>\n",
       "      <td>290.1207</td>\n",
       "      <td>0.8187</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.6752</td>\n",
       "      <td>0.8338</td>\n",
       "      <td>1.7413</td>\n",
       "      <td>0.7557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>79637</td>\n",
       "      <td>1224.710</td>\n",
       "      <td>533.1513</td>\n",
       "      <td>190.4367</td>\n",
       "      <td>80381</td>\n",
       "      <td>318.4289</td>\n",
       "      <td>0.9340</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>0.4888</td>\n",
       "      <td>0.6672</td>\n",
       "      <td>2.7996</td>\n",
       "      <td>0.5973</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>69647</td>\n",
       "      <td>1084.318</td>\n",
       "      <td>462.9416</td>\n",
       "      <td>191.8210</td>\n",
       "      <td>70216</td>\n",
       "      <td>297.7874</td>\n",
       "      <td>0.9101</td>\n",
       "      <td>0.9919</td>\n",
       "      <td>0.6002</td>\n",
       "      <td>0.7444</td>\n",
       "      <td>2.4134</td>\n",
       "      <td>0.6433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>87994</td>\n",
       "      <td>1210.314</td>\n",
       "      <td>507.2200</td>\n",
       "      <td>222.1872</td>\n",
       "      <td>88702</td>\n",
       "      <td>334.7199</td>\n",
       "      <td>0.8990</td>\n",
       "      <td>0.9920</td>\n",
       "      <td>0.7643</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>2.2828</td>\n",
       "      <td>0.6599</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>80011</td>\n",
       "      <td>1182.947</td>\n",
       "      <td>501.9065</td>\n",
       "      <td>204.7531</td>\n",
       "      <td>80902</td>\n",
       "      <td>319.1758</td>\n",
       "      <td>0.9130</td>\n",
       "      <td>0.9890</td>\n",
       "      <td>0.7374</td>\n",
       "      <td>0.7185</td>\n",
       "      <td>2.4513</td>\n",
       "      <td>0.6359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>84934</td>\n",
       "      <td>1159.933</td>\n",
       "      <td>462.8951</td>\n",
       "      <td>234.5597</td>\n",
       "      <td>85781</td>\n",
       "      <td>328.8485</td>\n",
       "      <td>0.8621</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.7360</td>\n",
       "      <td>0.7933</td>\n",
       "      <td>1.9735</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area  Perimeter  Major_Axis_Length  Minor_Axis_Length  Convex_Area  \\\n",
       "0     56276    888.242           326.1485           220.2388        56831   \n",
       "1     76631   1068.146           417.1932           234.2289        77280   \n",
       "2     71623   1082.987           435.8328           211.0457        72663   \n",
       "3     66458    992.051           381.5638           222.5322        67118   \n",
       "4     66107    998.146           383.8883           220.4545        67117   \n",
       "...     ...        ...                ...                ...          ...   \n",
       "2495  79637   1224.710           533.1513           190.4367        80381   \n",
       "2496  69647   1084.318           462.9416           191.8210        70216   \n",
       "2497  87994   1210.314           507.2200           222.1872        88702   \n",
       "2498  80011   1182.947           501.9065           204.7531        80902   \n",
       "2499  84934   1159.933           462.8951           234.5597        85781   \n",
       "\n",
       "      Equiv_Diameter  Eccentricity  Solidity  Extent  Roundness  \\\n",
       "0           267.6805        0.7376    0.9902  0.7453     0.8963   \n",
       "1           312.3614        0.8275    0.9916  0.7151     0.8440   \n",
       "2           301.9822        0.8749    0.9857  0.7400     0.7674   \n",
       "3           290.8899        0.8123    0.9902  0.7396     0.8486   \n",
       "4           290.1207        0.8187    0.9850  0.6752     0.8338   \n",
       "...              ...           ...       ...     ...        ...   \n",
       "2495        318.4289        0.9340    0.9907  0.4888     0.6672   \n",
       "2496        297.7874        0.9101    0.9919  0.6002     0.7444   \n",
       "2497        334.7199        0.8990    0.9920  0.7643     0.7549   \n",
       "2498        319.1758        0.9130    0.9890  0.7374     0.7185   \n",
       "2499        328.8485        0.8621    0.9901  0.7360     0.7933   \n",
       "\n",
       "      Aspect_Ration  Compactness  Class  \n",
       "0            1.4809       0.8207      0  \n",
       "1            1.7811       0.7487      0  \n",
       "2            2.0651       0.6929      0  \n",
       "3            1.7146       0.7624      0  \n",
       "4            1.7413       0.7557      0  \n",
       "...             ...          ...    ...  \n",
       "2495         2.7996       0.5973      1  \n",
       "2496         2.4134       0.6433      1  \n",
       "2497         2.2828       0.6599      1  \n",
       "2498         2.4513       0.6359      1  \n",
       "2499         1.9735       0.7104      1  \n",
       "\n",
       "[2500 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the Class column, and assign it back to the same column\n",
    "data['Class'] = label_encoder.fit_transform(data['Class'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the mapping of classes to their numerical labels\n",
    "label_mapping = dict(\n",
    "    zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label \"0\" has been assigned to Cervevelik, whilst label \"1\" has been assigned to Urgup Sivirsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = data.duplicated()\n",
    "num_duplicates = duplicates.sum()\n",
    "num_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no duplicates present in the dataset, there is no need to handle duplicate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Statistical Summary\n",
    "\n",
    "The goal of this stage is to figure out the descriptive statistics of the dataset by (a) computing the mean, median, standard deviation, minimum, and maximum for all numerical columns and (b) identifying outliers in features using interquartile range (IQR). This can be done using panda's describe() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics of Each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size / Area \n",
    "\n",
    "The mean area of the seeds is approximately 80,658 pixels, with a standard deviation of 13,664 pixels. This indicates a moderate variation in the sizes of the seeds. Most seeds are relatively similar in size, but there are outliers with areas as small as 47,939 pixels and as large as 136,574 pixels.\n",
    "\n",
    "The significant ranges for features like `Perimeter`, `Major_Axis_Length`, and `Equiv_Diameter` (with coefficients of variation indicating moderate spread) suggest the presence of some outlier seeds that deviate notably from the typical size and shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape Characteristics\n",
    "\n",
    "<b> Eccentricity </b> <br>\n",
    "The mean eccentricity is 0.8609, with a standard deviation of 0.0452, indicating that most seeds deviate moderately from a perfect circle. A minimum value of 0.4921 and a maximum of 0.9481 suggest variation in how elongated the seeds are.\n",
    "\n",
    "<b> Roundness </b> <br>\n",
    "With a mean of 0.7915 and an IQR of 0.0824, most seeds are relatively round but not perfectly circular. The minimum roundness value of 0.5546 highlights that some seeds are quite elongated.\n",
    "\n",
    "<b> Aspect Ratio </b> <br>\n",
    "The mean value of 2.0417 shows that most seeds are at least twice as long as their width, further supporting elongation in shape for some types.\n",
    "\n",
    "\n",
    "Therefore, based on the variation in these shape descriptors, it is likely that these features will be significant in distinguishing between the two classes, Class 0 (Cercevelik) and Class 1 (Urgup Sivrisi). For example, __Cercevelik__ may have higher Roundness and lower Eccentricity values (more circular) while __Urgup Sivrisi__ may exhibit higher Aspect_Ration and Eccentricity values (longer and thinner)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compactness and Solidity\n",
    "\n",
    "<b> Compactness </b> <br>\n",
    "The mean compactness is 0.7041, with a range from 0.5608 to 0.9049. This suggests variability in how closely the seeds approximate the area of a perfect circle with the same perimeter.\n",
    "\n",
    "<b> Solidity </b> <br>\n",
    "With a mean solidity of 0.9895 and a very small standard deviation (0.0035), most seeds are nearly convex, showing minimal indentations or irregularities.\n",
    "\n",
    "\n",
    "Therefore, along with size and shape, compactness may help in effectively classifying the seeds into their respective types. However, further statistical tests is needed to assess the separability of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics of Each Feature per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class0 = data[data['Class'] == 0]\n",
    "\n",
    "class0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = data[data['Class'] == 1]\n",
    "\n",
    "class1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size/Area\n",
    "Based on the features `Area`, `Perimeter`, and `Major_Axis_Length`, you can see how Class 1 (Urgup Sivrisi) seeds are much longer than Class 0 (Cercevelik) seeds. However, based on the feature `Minor_Axis_Length`, Class 0 (Cercevelik) seeds are considered to be wider than Class 1 (Urgup Sivrisi) seeds.\n",
    "\n",
    "### Shape Characteristics\n",
    "Based on the features `Roundness`, `Extent`, and `Compactness`, Class 0 (Cervecelik) seeds are more compact and rounder. Additionally, based on the features `Eccentricity` and `Aspect Ratio`, Class 1 (Urgup Sivrisi) seeds are longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Class Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the distribution of class labels in the dataset is a crucial step in understanding the balance of our data and ensuring that our machine learning models are trained effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = data['Class'].value_counts()\n",
    "distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates a fairly balanced dataset with just a 100 instance difference between the two classes. It is unlikely to significantly affect the machine learning models' performance. Both classes have a similar number of instances, which means that the models will likely have equal exposure to both classes during training. This reduces the risk of the models becoming biased toward the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Feature Distribution Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Area', 'Eccentricity', 'Roundness', 'Aspect_Ration', 'Compactness']\n",
    "classes = data['Class']\n",
    "\n",
    "# Visualize Feature Distributions\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(data=data, x=feature, hue='Class', kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {feature} by Class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graphs above, each of the features shows varying degrees of separation between Class 0 (Cercevelik) and Class 1 (Urgup Sivrisi). Aspect Ratio, Roundness, and Eccentricity provide particularly clear distinctions, suggesting that these shape-related features are strong indicators for classifying the seeds. Compactness and Area also contribute to the separability but with a bit more overlap.\n",
    "\n",
    "More specifically:\n",
    "\n",
    "- <b> Area Distribution </b> <br>\n",
    "Class 0 (Cercevelik) seeds generally have a lower area compared to Class 1 (Urgup Sivrisi) seeds, as indicated by the peak of the blue distribution on the left side of the plot.\n",
    "There is some overlap between the two classes, but Class 1 (Urgup Sivrisi) seeds have a wider range of areas and tend to have higher values overall.<br><br>\n",
    "- <b> Eccentricity Distribution </b> <br>\n",
    "Class 0 (Cercevelik) seeds have a slightly lower eccentricity on average compared to Class 1 (Urgup Sivrisi) seeds. This suggests that Class 1 (Urgup Sivrisi) seeds are more elongated, while Class 0 (Cercevelik) seeds are closer to circular shapes.\n",
    "The two classes are relatively well-separated in terms of eccentricity, indicating that this feature is useful for distinguishing between them.<br><br>\n",
    "- <b> Roundness Distribution </b> <br>\n",
    "Class 0 (Cercevelik) seeds generally have higher roundness values than Class 1 (Urgup Sivrisi) seeds, which supports the idea that Class 0 (Cercevelik) seeds are more circular.\n",
    "There is a noticeable distinction between the two classes, with limited overlap. This suggests that roundness is a good feature for class differentiation.<br><br>\n",
    "- <b> Aspect Ratio Distribution </b><br>\n",
    "Class 0 (Cercevelik) seeds tend to have lower aspect ratios, indicating they are closer to being circular or evenly proportioned in length and width.\n",
    "Class 1 (Urgup Sivrisi) seeds have higher aspect ratios, aligning with the description of them being more elongated.\n",
    "This feature has a clear separation between classes, making it highly effective for distinguishing between Class 0 (Cercevelik) and Class 1 (Urgup Sivrisi).<br><br>\n",
    "- <b> Compactness Distribution </b><br>\n",
    "Class 0 (Cercevelik) seeds have a higher compactness than Class 1 (Urgup Sivrisi) seeds, indicating they are more similar in shape to a perfect circle with a given perimeter.\n",
    "There is some overlap, but generally, the two classes show distinct peaks, suggesting that compactness can help in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how consistent the graphs are with the tables above as well. This gives us plausible key features to use in order to distinguish the two classes/seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c. Visual Comparison of Seeds via Drawing Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean for each class\n",
    "class0_major_axis = class0['Major_Axis_Length'].mean()\n",
    "class0_minor_axis = class0['Minor_Axis_Length'].mean()\n",
    "\n",
    "class1_major_axis = class1['Major_Axis_Length'].mean()\n",
    "class1_minor_axis = class1['Minor_Axis_Length'].mean()\n",
    "\n",
    "# Create the parametric points for the ovals\n",
    "t = np.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "# Oval for Class 0\n",
    "x0 = (class0_major_axis / 2) * np.cos(t)\n",
    "y0 = (class0_minor_axis / 2) * np.sin(t)\n",
    "plt.plot(x0, y0, label='Class 0 (Cercevelik)', color='blue', alpha=0.5)\n",
    "\n",
    "# Oval for Class 1\n",
    "x1 = (class1_major_axis / 2) * np.cos(t)\n",
    "y1 = (class1_minor_axis / 2) * np.sin(t)\n",
    "plt.plot(x1, y1, label='Class 1 (Urgup Sivrisi)', color='orange', alpha=0.5)\n",
    "\n",
    "# Add legends and labels\n",
    "plt.title(\"Seed Shapes per Class (Mean Values)\")\n",
    "plt.xlabel(\"X (scaled)\")\n",
    "plt.ylabel(\"Y (scaled)\")\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.gca().set_aspect('equal')  # Keep the aspect ratio equal\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5d. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64', 'int32'])\n",
    "\n",
    "correlation_matrix = numerical_columns.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Area`, `Convex_Area`, and `Equiv_Diameter` are highly correlated with each other (correlation close to 1). This is expected since these features are directly related to the size of the seed. Because they capture very similar information, including all these features in a model might lead to redundancy. Hence, it may be sufficient to keep just one of these in a model.\n",
    "\n",
    "`Perimeter` has a strong correlation with `Major_Axis_Length` (correlation of 0.95), suggesting that larger perimeter values are associated with greater major axis lengths. This also indicates some redundancy, as these features may be measuring similar aspects of seed size and shape.\n",
    "\n",
    "The Shape-related features show strong correlations with each other:\n",
    "- `Eccentricity` and `Aspect_Ration` have a high positive correlation (0.95), meaning as eccentricity increases, so does the aspect ratio, which aligns with elongated shapes.\n",
    "- `Eccentricity` and `Compactness` have a strong negative correlation (-0.98), indicating that more eccentric shapes (more elongated) have lower compactness.\n",
    "- `Roundness` is also strongly correlated with `Aspect_Ration` (-0.94) and `Compactness` (0.93), showing that rounder seeds have a lower aspect ratio and higher compactness.\n",
    "\n",
    "Given the high correlations among `Eccentricity`, `Aspect_Ration`, and `Compactness`, it may be another thing to consider by just choosing one or two of these to avoid redundancy.\n",
    "\n",
    "\n",
    "`Solidity` and `Extent` have generally low correlations with other features, meaning they may capture unique aspects of seed shape or compactness that are not covered by other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, features such as the `Eccentricity` and `Aspect_Ration` show a strong positive correlation with the target class while `Roundness`, `Compactness` show a strong negative correlation. Moreover, `Major_Axis_Length` shows a moderate positive correlation while `Minor_Axis_Length` shows a moderate `negative correlation` meaning both have a noticeable impact but may require other features or interactions for better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5e. Pairwise Scatter Plots\n",
    "Based on the Pairwise Scatter Plots below, the features `Eccentricity`, `Roundness`, and `Aspect_Ration` show strong separability between Class 0 (Cercevelik) and Class 1 (Urgup Sivrisi), making them effective for classification. `Compactness` and `Area` also contribute to separability but to a lesser extent. The combination of shape-related features (like `Eccentricity`, `Roundness`, and `Aspect_Ration`) appears to be especially valuable for distinguishing the two classes, suggesting that shape is a more defining factor for classification than size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Scatter Plots\n",
    "sns.pairplot(df[features + ['Class']], hue='Class', diag_kind='kde', height=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5f. Principal Component Analysis\n",
    "The PCA (Principal Component Analysis) plot provides a two-dimensional representation of the data by reducing the features to two principal components (PCA1 and PCA2), which capture the most variance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "features = ['Eccentricity', 'Roundness', 'Aspect_Ration', 'Compactness']\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df[features])\n",
    "df['PCA1'], df['PCA2'] = pca_result[:, 0], pca_result[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='PCA1', y='PCA2', hue='Class', style='Class', s=100)\n",
    "plt.title('PCA: Feature Separability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_\n",
    "feature_contributions = pd.DataFrame(components.T, index=features, columns=[\n",
    "                                     f'PC{i+1}' for i in range(len(components))])\n",
    "\n",
    "# Check absolute contributions of features to the first few principal components\n",
    "print(\"Feature Contributions:\")\n",
    "print(feature_contributions)\n",
    "\n",
    "feature_importance = feature_contributions.abs().sum(\n",
    "    axis=1).sort_values(ascending=False)\n",
    "print(\"\\nFeature Importance (Summed Across PCs):\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Eccentricity`, `Roundness`, `Aspect_Ration`, and `Compactness` are moderately effective at distinguishing Class 0 (Cercevelik) and Class 1 (Urgup Sivrisi), as seen by the general clustering of each class on opposite sides of PCA1. However, due to the overlap in the middle, these features alone may not perfectly separate the classes. Including additional features (such as `Area`) or exploring non-linear methods might further enhance separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "features = ['Area','Eccentricity', 'Roundness', 'Aspect_Ration', 'Compactness']\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df[features])\n",
    "df['PCA1'], df['PCA2'] = pca_result[:, 0], pca_result[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='PCA1', y='PCA2', hue='Class', style='Class', s=100)\n",
    "plt.title('PCA: Feature Separability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_\n",
    "feature_contributions = pd.DataFrame(components.T, index=features, columns=[\n",
    "                                     f'PC{i+1}' for i in range(len(components))])\n",
    "\n",
    "# Check absolute contributions of features to the first few principal components\n",
    "print(\"Feature Contributions:\")\n",
    "print(feature_contributions)\n",
    "\n",
    "feature_importance = feature_contributions.abs().sum(\n",
    "    axis=1).sort_values(ascending=False)\n",
    "print(\"\\nFeature Importance (Summed Across PCs):\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding `Area` to the set of features (alongside `Eccentricity`, `Roundness`, `Aspect_Ration`, and `Compactness`) has improved the separability of the Class 0 (Cercevelik) and Class 1 (Urgup Sivrisi) classes in the PCA plot. This suggests that both size (represented by Area) and shape-related features are important for distinguishing these classes effectively. The improvement in separability implies that a classification model trained on this combination of features would likely perform better than one trained without Area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "features = ['Area','Eccentricity', 'Roundness']\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df[features])\n",
    "df['PCA1'], df['PCA2'] = pca_result[:, 0], pca_result[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='PCA1', y='PCA2', hue='Class', style='Class', s=100)\n",
    "plt.title('PCA: Feature Separability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_\n",
    "feature_contributions = pd.DataFrame(components.T, index=features, columns=[\n",
    "                                     f'PC{i+1}' for i in range(len(components))])\n",
    "\n",
    "# Check absolute contributions of features to the first few principal components\n",
    "print(\"Feature Contributions:\")\n",
    "print(feature_contributions)\n",
    "\n",
    "feature_importance = feature_contributions.abs().sum(\n",
    "    axis=1).sort_values(ascending=False)\n",
    "print(\"\\nFeature Importance (Summed Across PCs):\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Area`, `Eccentricity`, and `Roundness` are effective at distinguishing Class 0 (Cercevelik) and Class 1 (Urgup Sivrisi), showing a clear seperation along PCA2. The removal of `Aspect_Ration` and `Compactness` provided a cleaner seperation of the two classes, and reduced the noise. This suggests that both size (represented by Area), and shape-related features are important for distinguishing these classes. However, there is still room for improvement as some overlap still exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "features = ['Eccentricity', 'Roundness']\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df[features])\n",
    "df['PCA1'], df['PCA2'] = pca_result[:, 0], pca_result[:, 1]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='PCA1', y='PCA2', hue='Class', style='Class', s=100)\n",
    "plt.title('PCA: Feature Separability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.components_\n",
    "feature_contributions = pd.DataFrame(components.T, index=features, columns=[\n",
    "                                     f'PC{i+1}' for i in range(len(components))])\n",
    "\n",
    "# Check absolute contributions of features to the first few principal components\n",
    "print(\"Feature Contributions:\")\n",
    "print(feature_contributions)\n",
    "\n",
    "feature_importance = feature_contributions.abs().sum(\n",
    "    axis=1).sort_values(ascending=False)\n",
    "print(\"\\nFeature Importance (Summed Across PCs):\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Eccentricity` and `Roundness` are effective at distinguishing Class 0 (Cercevelik) and Class 1 (Urgup Sivrisi), showing clear separation along PCA2. The removal of `Area`, `Aspect_Ratio`, and `Compactness` simplified the feature set while maintaining meaningful class separation. This suggests that shape-related features, particularly Eccentricity and Roundness, play a key role in distinguishing these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6. Initial Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we perform the initial model training to try to identify the underlying patterns in the dataset. **k-Nearest Neighbors (kNN), Logistic Regression, and Naive Bayes** are the three models chosen to perform the classification task of identifying the type of pumpkin seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Exploratory Data Analysis findings, features such as `Eccentricity`, `Aspect_Ration`, `Roundness`, and `Compactness` have a strong correlation to the target class so these are highly predictive of the target class and can be used as features when fed to the model. In addition, features with moderate correlation with the target class such as the `Major_Axis_Length` and the `Minor_Axis_Length` may also be used since both may help predict the target class when combined with the features with strong correlation. Lastly, based on the PCA output, `Area` can also be added as it has improved the seperability of the target class when combined with `Eccentricity`, `Aspect_Ration`, `Roundness`, and `Compactness`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature and Target Selection\n",
    "features = data[['Eccentricity', 'Aspect_Ration', 'Roundness', 'Compactness', 'Major_Axis_Length',  'Minor_Axis_Length', 'Area']]\n",
    "target = data['Class']\n",
    "\n",
    "# Feature and Target Selection\n",
    "features_nn = data.drop(columns=['Class'])  # All columns except 'Class'\n",
    "target_nn = data['Class']  # The target column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting of Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = target\n",
    "X_nn = features_nn\n",
    "y_nn = target_nn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling is the act of standardizing or normalizing the range of features in the dataset. This is done as kNN, Logistic Regression and Neural Networks are sensitive to the magnitude of feature values. If features were kept as is, having different scales, these models may prioritize larger valued features, and ignore the small ones. In our dataset, most of the size features could easily overthrow the shape features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled_nn = scaler.fit_transform(X_train_nn)\n",
    "X_test_scaled_nn = scaler.transform(X_test_nn)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_train_scaled_nn = pd.DataFrame(X_train_scaled_nn, columns=X_train_nn.columns, index=X_train_nn.index)\n",
    "\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "X_test_scaled_nn = pd.DataFrame(X_test_scaled_nn, columns=X_test_nn.columns, index=X_test_nn.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors (kNN) Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple machine learning model with relitvely high interpretability. The model makes no prior assumptions about the data distribution and its outputs are easily interpretable. KNN's predictions are made based on the class majority of the closest neighbors. The closest neighbors is a hyperparameter `n_neighbors`, which by default is set to three. Having a smaller number of neighbores makes the model less sensitive to noise, while having a higher value gives a clearer decision boundary.  \n",
    "\n",
    "\n",
    "To measure the distance between the datapoints, KNN uses the _Eucledian distance_ by default. Other distance metrics could be used such as but not limited to the _Minowski distance_, _Manhattan distance_, and the _Jaccard distance_. The _Eucledian distance_ will be used in this model, since the scaled data can have negative values, making the _Minowski distance_ unusable. \n",
    "\n",
    "\n",
    "The default weight setting of the model is `uniform` making all neighbors contribute equally to the prediction. \n",
    "\n",
    "\n",
    "To tune this model we need to find the best value for `n_neighbors`. The best way to do this is by using the elbow method.\n",
    "\n",
    "We chose kNN as the model excels in identify local patterns and boundaries, which is great as the 2 different classes seem to have a concise difference. This is also good with a small set of features, which we may do at the latter part of the paper as shown by the PCA results. Also it is simple to implement, and we wanted to show how a simple model would perform beside the 2 models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;KNeighborsClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "model_knn_scaled = KNeighborsClassifier(n_neighbors=3)\n",
    "model_knn_scaled.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is designed for binary classification tasks, perfect for this tak. This model predicts the probability for the target class using a _sigmoid function_. This model assumes that there is a linear relationship between the features and the log-probability of the target classes. The `regularization rate` is a hyperparameter which is used to mitigate overfitting. By default, the _L2_ regularization is applied, which penalizes inputs with large coefficients. This helps keep the model more generalized.\n",
    "\n",
    "To control the regularization, another hyperparameter `c` handles the penalty for regularization. By default, this value is set to `1.0`. Increasing this value results to a more flexible model, while decreasing this value makes the model more stiff.\n",
    "\n",
    "This model was chosen as its very simple and straightforward to implement, like kNN, but this time, it is able to assign weights to important features. This is essential as found in the PCA results that `Eccentricity` and `Roundness` do really well in seperation. Ultimately, this model is really good with binary classification, perfect for classifying the two seeds. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=200, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=200, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=200, random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "model_lr_scaled = LogisticRegression()\n",
    "model_lr_scaled.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks is a highly flexible model which can capture complex and non-linear relationships between the data. It performs well on datasets with large number of features or high-dimensionality. \n",
    "The output layer for this can be set into _Sigmoid_ which is the same function used in Logistic Regression.\n",
    "\n",
    "The number of hidden layers and neurons of the network is one of this model's hyperparameters. The number of hidden layers and neurons depends on the complexity of the data and the patterns it needs to capture.\n",
    "\n",
    "Another important hyperparameter is the activation function used in the hidden layers. The _ReLU_ (Rectified Linear Unit) activation function is commonly chosen for hidden layers due to its efficiency and ability to introduce non-linearity into the model, allowing it to capture complex patterns. The Sigmoid activation function, used in the output layer, is ideal for binary classification as it outputs a probability score between 0 and 1, making it easy to threshold for binary decisions.\n",
    "\n",
    "The optimizer used to train the neural network is another critical hyperparameter. For this task, the _Adam optimizer_ is chosen, which combines the advantages of both _RMSprop_ and _Stochastic Gradient Descent (SGD)_. It adapts the learning rate dynamically during training, ensuring faster and more stable convergence. The initial learning rate is set to `0.001`, which is a common default that provides a balance between convergence speed and stability.\n",
    "\n",
    "To prevent overfitting, regularization is applied using an _L2 penalty_ (weight decay) with a strength parameter (alpha) of `0.0001`. This ensures that the model does not assign excessive importance to any single feature, helping to generalize better to unseen data. Additionally, early stopping can be used during training to halt the process when the model’s performance on a validation set stops improving, avoiding unnecessary epochs and overfitting.\n",
    "\n",
    "The batch size controls the number of samples processed before updating the model parameters. A batch size of 32 is commonly used as it provides a good balance between memory efficiency and convergence speed. Finally, the model is trained for a maximum of 100 epochs, although early stopping can terminate the training earlier if the validation loss plateaus.\n",
    "\n",
    "This was chosen as our last model as it is very flexible and can learn interactions between features. We would also want to see how a complex model can fare with a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = MLPClassifier(\n",
    "    hidden_layer_sizes=(8, ),\n",
    "    activation='relu',\n",
    "    alpha=0.0001,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=100,\n",
    "    random_state=42)\n",
    "model_nn.fit(X_train_nn, y_train_nn)\n",
    "\n",
    "model_nn_scaled = MLPClassifier(\n",
    "    hidden_layer_sizes=(8, ),\n",
    "    activation='relu',\n",
    "    alpha=0.0001,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=100,\n",
    "    random_state=42)\n",
    "model_nn_scaled.fit(X_train_scaled_nn, y_train_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "y_pred_knn_train_unscaled = model_knn.predict(X_train)\n",
    "print('KNN Model (Unscaled - Training Set)')\n",
    "print(classification_report(y_train, y_pred_knn_train_unscaled))\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred_knn_train_unscaled)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for KNN (Unscaled - Training Set):\")\n",
    "print(cm)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Test Set\n",
    "y_pred_knn_test_unscaled = model_knn.predict(X_test)\n",
    "print('KNN Model (Unscaled - Test Set)')\n",
    "print(classification_report(y_test, y_pred_knn_test_unscaled))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_knn_test_unscaled)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"], \n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for KNN (Unscaled - Test Set):\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "y_pred_knn_train_scaled = model_knn_scaled.predict(X_train_scaled)\n",
    "print('KNN Model (Scaled - Training Set)')\n",
    "print(classification_report(y_train, y_pred_knn_train_scaled))\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred_knn_train_scaled)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for KNN (Scaled - Training Set):\")\n",
    "print(cm)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Test Set\n",
    "y_pred_knn_test_scaled = model_knn_scaled.predict(X_test_scaled)\n",
    "print('KNN Model (Scaled - Test Set)')\n",
    "print(classification_report(y_test, y_pred_knn_test_scaled))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_knn_test_scaled)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for KNN (Scaled - Test Set):\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Scores for Training and Test Sets\n",
    "\n",
    "# Unscaled\n",
    "knn_train_acc_us = accuracy_score(y_train, y_pred_knn_train_unscaled)\n",
    "knn_test_acc_us = accuracy_score(y_test, y_pred_knn_test_unscaled)\n",
    "print(f\"kNN Training Accuracy: {knn_train_acc_us * 100:.2f}%\")\n",
    "print(f\"kNN Test Accuracy: {knn_test_acc_us * 100:.2f}%\")\n",
    "\n",
    "# Scaled\n",
    "knn_train_acc_s = accuracy_score(y_train, y_pred_knn_train_scaled)\n",
    "knn_test_acc_s = accuracy_score(y_test, y_pred_knn_test_scaled)\n",
    "print(f\"kNN Training Accuracy (Scaled): {knn_train_acc_s * 100:.2f}%\")\n",
    "print(f\"kNN Test Accuracy (Scaled): {knn_test_acc_s * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling had a significant positive impact on the KNN model's performance. When the data was scaled, both training and test accuracies improved substantially, with the training accuracy increasing from 89.00% to 91.75% and the test accuracy jumping from 67.40% to 85.20%. Additionally, precision, recall, and F1-scores for both classes were higher with the scaled data, highlighting the importance of feature scaling for models like KNN, which rely on distance metrics for predictions.\n",
    "\n",
    "The unscaled model exhibited signs of overfitting, as evidenced by the large gap between its training accuracy (89.00%) and test accuracy (67.40%). In contrast, the scaled model mitigated this issue, reducing the performance gap between training and test sets (91.75% vs. 85.20%). This indicates that scaling not only improved the model's accuracy but also enhanced its ability to generalize to unseen data.\n",
    "\n",
    "Analyzing the confusion matrices further supports the effectiveness of scaling. The scaled model consistently had fewer misclassifications on both the training and test sets compared to the unscaled version. This demonstrates that scaling improves the model's ability to correctly classify instances, particularly in scenarios where features vary in magnitude. Overall, the results show that feature scaling is essential for achieving optimal performance with KNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model (Unscaled - Training Set)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89      1049\n",
      "           1       0.89      0.85      0.87       951\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n",
      "Confusion Matrix for Logistic Regression (Unscaled - Training Set):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0          950           99\n",
      "Actual 1          141          810\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression Model (Unscaled - Test Set)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       251\n",
      "           1       0.85      0.81      0.83       249\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.84      0.84      0.84       500\n",
      "weighted avg       0.84      0.84      0.84       500\n",
      "\n",
      "Confusion Matrix for Logistic Regression (Unscaled - Test Set):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0          216           35\n",
      "Actual 1           47          202\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Evaluation (Unscaled)\n",
    "# Training Set\n",
    "y_pred_lr_train_unscaled = model_lr.predict(X_train)\n",
    "print('Logistic Regression Model (Unscaled - Training Set)')\n",
    "print(classification_report(y_train, y_pred_lr_train_unscaled))\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred_lr_train_unscaled)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for Logistic Regression (Unscaled - Training Set):\")\n",
    "print(cm)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "y_pred_lr_test_unscaled = model_lr.predict(X_test)\n",
    "print('Logistic Regression Model (Unscaled - Test Set)')\n",
    "print(classification_report(y_test, y_pred_lr_test_unscaled))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_lr_test_unscaled)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for Logistic Regression (Unscaled - Test Set):\")\n",
    "print(cm)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model (Scaled - Training Set)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89      1049\n",
      "           1       0.89      0.85      0.87       951\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n",
      "Confusion Matrix for Logistic Regression (Scaled - Training Set):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0          953           96\n",
      "Actual 1          142          809\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression Model (Scaled - Test Set)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84       251\n",
      "           1       0.86      0.81      0.83       249\n",
      "\n",
      "    accuracy                           0.84       500\n",
      "   macro avg       0.84      0.84      0.84       500\n",
      "weighted avg       0.84      0.84      0.84       500\n",
      "\n",
      "Confusion Matrix for Logistic Regression (Scaled - Test Set):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0          218           33\n",
      "Actual 1           47          202\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Set\n",
    "y_pred_lr_scaled_train = model_lr_scaled.predict(X_train_scaled)\n",
    "print('Logistic Regression Model (Scaled - Training Set)')\n",
    "print(classification_report(y_train, y_pred_lr_scaled_train))\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred_lr_scaled_train)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for Logistic Regression (Scaled - Training Set):\")\n",
    "print(cm)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Test Set\n",
    "y_pred_lr_scaled_test = model_lr_scaled.predict(X_test_scaled)\n",
    "print('Logistic Regression Model (Scaled - Test Set)')\n",
    "print(classification_report(y_test, y_pred_lr_scaled_test))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_lr_scaled_test)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for Logistic Regression (Scaled - Test Set):\")\n",
    "print(cm)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Accuracy: 88.00%\n",
      "Logistic Regression: 83.60%\n",
      "Logistic Regression Training Accuracy (Scaled): 88.10%\n",
      "Logistic Regression Test Accuracy (Scaled): 84.00%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Scores for Training and Test Sets\n",
    "\n",
    "# Unscaled\n",
    "lr_train_acc_us = accuracy_score(y_train, y_pred_lr_train_unscaled)\n",
    "lr_test_acc_us = accuracy_score(y_test, y_pred_lr_test_unscaled)\n",
    "print(f\"Logistic Regression Training Accuracy: {lr_train_acc_us * 100:.2f}%\")\n",
    "print(f\"Logistic Regression: {lr_test_acc_us * 100:.2f}%\")\n",
    "\n",
    "# Scaled\n",
    "lr_train_acc_s = accuracy_score(y_train, y_pred_lr_scaled_train)\n",
    "lr_test_acc_s = accuracy_score(y_test, y_pred_lr_scaled_test)\n",
    "print(f\"Logistic Regression Training Accuracy (Scaled): {lr_train_acc_s * 100:.2f}%\")\n",
    "print(f\"Logistic Regression Test Accuracy (Scaled): {lr_test_acc_s * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the features led to a slight improvement in the performance of the logistic regression model. While the training accuracy increased marginally from 87.90% to 88.05%, the test accuracy saw a more notable improvement from 82.60% to 84.00%. This indicates that scaling helps the model generalize better to unseen data, though the magnitude of the improvement is less dramatic compared to distance-based models like KNN.\n",
    "\n",
    "The confusion matrices and classification reports provide deeper insights. For both the unscaled and scaled models, the training performance was relatively stable, with similar precision, recall, and F1-scores for both classes. However, the scaled model showed fewer misclassifications on the test set. For instance, the scaled test set confusion matrix shows a reduction in false positives and false negatives compared to the unscaled version, with only 33 Class 0 samples misclassified as Class 1 (down from 38) and 47 Class 1 samples misclassified as Class 0 (down from 49). These improvements suggest that scaling features contributes to slightly better boundary learning in the logistic regression model.\n",
    "\n",
    "Despite the modest gains, the logistic regression model already performs consistently well regardless of scaling, achieving over 83% test accuracy in both cases. This is expected because logistic regression is less sensitive to feature scaling than KNN. However, scaling still enhances model stability and slightly boosts generalization performance, making it a good practice when working with datasets where feature magnitudes vary significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "y_pred_nn_train_unscaled = model_nn.predict(X_train_nn)\n",
    "print('Neural Network Model (Unscaled - Training Set)')\n",
    "print(classification_report(y_train_nn, y_pred_nn_train_unscaled))\n",
    "\n",
    "cm = confusion_matrix(y_train_nn, y_pred_nn_train_unscaled)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for Neural Network (Unscaled - Training Set):\")\n",
    "print(cm)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Test Set\n",
    "y_pred_nn_test_unscaled = model_nn.predict(X_test_nn)\n",
    "print('Neural Network Model (Unscaled - Test Set)')\n",
    "print(classification_report(y_test_nn, y_pred_nn_test_unscaled))\n",
    "\n",
    "cm = confusion_matrix(y_test_nn, y_pred_nn_test_unscaled)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for Neural Network (Unscaled - Test Set):\")\n",
    "print(cm)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set\n",
    "y_pred_nn_train_scaled = model_nn_scaled.predict(X_train_scaled_nn)\n",
    "print('Neural Network Model (Scaled - Training Set)')\n",
    "print(classification_report(y_train_nn, y_pred_nn_train_scaled))\n",
    "\n",
    "cm = confusion_matrix(y_train_nn, y_pred_nn_train_scaled)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for Neural Network (Scaled - Training Set):\")\n",
    "print(cm)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Test Set\n",
    "y_pred_nn_test_scaled = model_nn_scaled.predict(X_test_scaled_nn)\n",
    "print('Neural Network Model (Scaled - Test Set)')\n",
    "print(classification_report(y_test_nn, y_pred_nn_test_scaled))\n",
    "\n",
    "cm = confusion_matrix(y_test_nn, y_pred_nn_test_scaled)\n",
    "cm = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(\"Confusion Matrix for Neural Network (Scaled - Test Set):\")\n",
    "print(cm)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Scores for Training and Test Sets\n",
    "\n",
    "# Unscaled\n",
    "nn_train_acc_us = accuracy_score(y_train_nn, y_pred_nn_train_unscaled)\n",
    "nn_test_acc_us = accuracy_score(y_test_nn, y_pred_nn_test_unscaled)\n",
    "print(f\"Neural Network Training Accuracy: {nn_train_acc_us * 100:.2f}%\")\n",
    "print(f\"Neural Network Test Accuracy: {nn_test_acc_us * 100:.2f}%\")\n",
    "\n",
    "# Scaled\n",
    "nn_train_acc_s = accuracy_score(y_train_nn, y_pred_nn_train_scaled)\n",
    "nn_test_acc_s = accuracy_score(y_test_nn, y_pred_nn_test_scaled)\n",
    "print(f\"Neural Network Training Accuracy (Scaled): {nn_train_acc_s * 100:.2f}%\")\n",
    "print(f\"Neural Network Test Accuracy (Scaled): {nn_test_acc_s * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Edit markdown below to match fixes done above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling had a dramatic impact on the performance of the neural network model. Without scaling, the model performed poorly, with a training accuracy of 44.95% and a test accuracy of 45.40%. These results indicate that the neural network struggled to learn meaningful patterns from the unscaled features. The classification reports reveal that the unscaled model had high recall for Class 0 (80% for the training set and 85% for the test set), but very poor recall for Class 1 (7% for training and 5% for test). This imbalance suggests that the model was biased toward predicting Class 0, as evidenced by the high number of false negatives for Class 1 in the confusion matrices.\n",
    "\n",
    "\n",
    "After scaling the features, the neural network's performance improved significantly. Training accuracy rose to 87.90%, and test accuracy increased to 83.00%. The scaled model demonstrated much better recall for Class 1 (85% for training and 80% for test), though it remained lower than the recall for Class 0 (96% for training and 94% for test). The precision for Class 1 also improved substantially (from 23% unscaled to 87% scaled on the training set and from 26% unscaled to 82% scaled on the test set). This improvement suggests that scaling allowed the neural network to better differentiate between the two classes and reduce its bias toward predicting Class 0.\n",
    "\n",
    "The confusion matrices for the scaled model provide additional insights. For the test set, the number of correctly classified Class 0 instances increased (235 compared to 214 unscaled), while the number of correctly classified Class 1 instances also improved significantly (75 compared to just 13 unscaled). However, a considerable number of Class 1 instances were still misclassified as Class 0, indicating that while scaling improved performance, the model's ability to accurately classify Class 1 remains limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8. Improving model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Finding the Best Value for K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for hyperparameter tuning for kNN is to find the optimal value for k. The evaluation metrics: accuracy, precision, recall, and F1-score, will be used to assess the impact of hyperparameter tuning. This segment will make use of `Grid Search` to find the best combination of hyper parameters. In this case we are just searching for the best value for `k` or the `n_neighbors` hyperparameter.    \n",
    "\n",
    "`GridSearchCV` exhaustively tests the possible combinations for the hyper parameters set in `grid_params_knn` with a 10-fold cross validation. This ensures that the model is evaluatied on multiple subsets of the training data to try and mitigate the overfitting issues discovered in `section 7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params_knn = { 'n_neighbors': range(1, 10) }\n",
    "grid_search_knn= GridSearchCV(KNeighborsClassifier(), grid_params_knn, cv=10)\n",
    "grid_search_knn.fit(X_train_scaled, y_train)\n",
    "print(f\"Best Parameters GridSearch: {grid_search_knn.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the mean error for the test scores in relation to k over time\n",
    "\n",
    "# Extracting the mean test scores for hyperparameter k\n",
    "mean_test_scores = grid_search_knn.cv_results_['mean_test_score']\n",
    "param_range = grid_params_knn['n_neighbors']\n",
    "best_n_neighbors = grid_search_knn.best_params_['n_neighbors']\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(param_range, mean_test_scores, marker='o', linestyle='-', label='Mean CV Accuracy')\n",
    "plt.axvline(best_n_neighbors, color='red', linestyle='--', label='Best n_neighbors')\n",
    "plt.title('GridSearchCV Results for kNN (n_neighbors)', fontsize=14)\n",
    "plt.xlabel('Number of Neighbors (n_neighbors)', fontsize=12)\n",
    "plt.ylabel('Mean CV Accuracy', fontsize=12)\n",
    "plt.xticks(param_range)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn_tuned = grid_search_knn.best_estimator_\n",
    "model_knn_tuned.fit(X_train_scaled, y_train)\n",
    "score_knn_tuned = model_knn_tuned.score(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for tuned kNN:\")\n",
    "print(f\"Test Accuracy: {score_knn_tuned * 100:.2f}%\")\n",
    "print(classification_report(y_test, model_knn_tuned.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_knn_untuned = model_knn_scaled.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Test Accuracy for scaled KNN without tuning: {score_knn_untuned * 100:.2f}%\")\n",
    "print(\"Confusion Matrix for scaled kNN: without tuning\")\n",
    "print(classification_report(y_test, model_knn_scaled.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` identified that `8` is the best value for the number of neighbots. Using this, the knn model performance showed slight improvements in the model's accuracy. The model showed a `1.6% increase in accuracy`. Additionally, the model makes fewer `false positives` for class 1 and fewer `false negatives` for class 0. This indicates that the model is able to make fewer mistakes in classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Finding the Best Value for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for `Logistic Regression` is to improve the model performance using `GridSearchCV`. The hyperparameters used are `penalty`, `C`, `solver`, and `max_iter`. Like what was done in the KNN segment of tuning, this segment will exhaustively go through the combinations of hyperparameters and return the best performing combination. The `penalty` parameter is the type of regularization to be used. In this case, `l2` is the only a parameters which works with the given solver. The `C` parameter is the regularization strength, this dictates how flexible the model can be. Having a smaller value will imply a stronger regularization, while a higher value leads to the opposite. By exploring values from (0.0001 to 10,000) we can try to find the best strength for regularization. The `solver` parameter is the optimization algorithms to be used. Finally, `max_iter` is the parameter for the number of iterations for convergence for the solver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lr = {\n",
    "    'penalty': ['l2'],\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag'],\n",
    "    'max_iter': [500, 1000, 1500, 2000]\n",
    "}\n",
    "grid_search_lr = GridSearchCV(LogisticRegression(), params_lr, cv=10, n_jobs=-1, verbose=2)\n",
    "grid_search_lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Parameters GridSearch: {grid_search_lr.best_params_}\")\n",
    "print(f\"Best Score GridSearch: {grid_search_lr.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results_lr = grid_search_lr.cv_results_\n",
    "\n",
    "C_vals = params_lr['C']\n",
    "best_C = grid_search_lr.best_params_['C']\n",
    "mean_test_scores = grid_results_lr['mean_test_score']\n",
    "std_test_scores = grid_results_lr['std_test_score']\n",
    "selected_solver = 'sag'\n",
    "selected_max_iter = 1000\n",
    "filtered_scores = [\n",
    "    mean_test_scores[i] for i in range(len(mean_test_scores))\n",
    "    if grid_results_lr['param_solver'][i] == selected_solver\n",
    "    and grid_results_lr['param_max_iter'][i] == selected_max_iter\n",
    "]\n",
    "\n",
    "filtered_std = [\n",
    "    std_test_scores[i] for i in range(len(std_test_scores))\n",
    "    if grid_results_lr['param_solver'][i] == selected_solver\n",
    "    and grid_results_lr['param_max_iter'][i] == selected_max_iter\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(C_vals, filtered_scores, marker='o', label='Mean Accuracy')\n",
    "plt.fill_between(\n",
    "    C_vals,\n",
    "    np.array(filtered_scores) - np.array(filtered_std),\n",
    "    np.array(filtered_scores) + np.array(filtered_std),\n",
    "    alpha=0.2,\n",
    "    label= 'Std Dev')\n",
    "plt.axvline(x=best_C, color='red', linestyle='--', label=f'Best C = {best_C:.4f}')\n",
    "plt.title('Hyperparameter tuning for Logistic Regression: Accuracy vs C', fontsize=14)\n",
    "plt.xlabel('Regularization Strength (C)', fontsize=12)\n",
    "plt.ylabel('Mean CV Accuracy', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean test scores for `max_iter`\n",
    "max_iter_values = params_lr['max_iter']\n",
    "best_max_iter = grid_search_lr.best_params_['max_iter']\n",
    "mean_test_scores_max_iter = [\n",
    "    np.mean([\n",
    "        mean_test_scores[i] for i in range(len(mean_test_scores))\n",
    "        if grid_results_lr['param_max_iter'][i] == max_iter\n",
    "    ])\n",
    "    for max_iter in max_iter_values\n",
    "]\n",
    "\n",
    "# Plot accuracy vs. max_iter\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_iter_values, mean_test_scores_max_iter, marker='o', label='Mean CV Accuracy')\n",
    "plt.axvline(x=best_max_iter, color='red', linestyle='--', label=f'Best max_iter = {best_max_iter}')\n",
    "plt.title(\"Mean Test Accuracy vs Max Iterations (Cross-Validation)\")\n",
    "plt.xlabel(\"Max Iterations\")\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define specific solvers and ensure results are grouped by C\n",
    "solvers = params_lr['solver']\n",
    "\n",
    "# Loop through each solver\n",
    "for solver in solvers:\n",
    "    # Filter for the current solver\n",
    "    filtered_scores_solver = [\n",
    "        np.mean([\n",
    "            mean_test_scores[i]\n",
    "            for i in range(len(mean_test_scores))\n",
    "            if grid_results_lr['param_solver'][i] == solver\n",
    "            and grid_results_lr['param_C'][i] == C\n",
    "        ])\n",
    "        for C in C_vals\n",
    "    ]\n",
    "\n",
    "    # Plot the accuracy for each solver\n",
    "    plt.semilogx(C_vals, filtered_scores_solver, marker='o', label=f'Solver: {solver}')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.title(\"Accuracy vs Regularization (C) for Different Solvers\")\n",
    "plt.xlabel(\"Regularization Strength (C)\")\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_tuned = grid_search_lr.best_estimator_\n",
    "model_lr_tuned.fit(X_train_scaled, y_train)\n",
    "score_lr_tuned = model_lr_tuned.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for tuned logistic regression:\")\n",
    "print(f\"Test Accuracy: {score_lr_tuned * 100:.2f}%\")\n",
    "print(classification_report(y_test, model_lr_tuned.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_lr_untuned = model_lr_scaled.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"Confusion Matrix for scaled logistic regression: without tuning\")\n",
    "print(f\"Test Accuracy: {score_lr_untuned * 100:.2f}%\")\n",
    "print(classification_report(y_test, model_lr_scaled.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning the logistic regression model improved its accuracy by `1.4%`. There was also increases in precision for both the positive class `1` and the negative class `0`, suggesting that the model is able to identify these classes while reducing false positives. Additionaly, the model is better at identifying true positive classes due to an incrase in recall for the positive class. Finally, the increase in `f1-score` for the both classes indicates that the model has improved its balance between precision and recall, making it more effective at handling tricker test instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the neural network we will tune the hyperparameters by systematically searching through a predefined range of the hyperparameters we want using `RandomizedSearchCV`. In the previous models, `GridSearchCV` was used since it would not be as computationally intensive to go through the hyperparameters set. However, due to the nature of neural networks, the exhaustively searching for the best parameters takes too long. Instead the group opted to use `RandomizedSearchCV` to reduce the time complexity of tuning the hyperparameters for this model. This search algorithm works by sampling a fix number of random combinations from the parameters set. This makes it faster and more coputationally efficient for large parameter spaces, however it does not guarantee that the parameters given would be the best one. \n",
    "\n",
    "The `hidden_layer_size` represents the number of neurons in each layer. Tuning this parameter will allow us to test various levels of complexity of the network. \n",
    "\n",
    "The `activation` parameter sets the activation function for each neuron. `tanh` is good for balanced data while `relu` is good for avoiding the vanishing gradient problem. \n",
    "\n",
    "The `alpha` parameter is the regularization stength. Similar to the logistic regression, tuning this parameter helps us ensure the model captures the data well by trying not to overfit and underfit. Exploring values from (0.00001 to 0.01) or `np.logspace(-4, -2, 5)`, lets us gradually increase the strength of regularization without having too high of a jump.\n",
    "\n",
    "The `learning_rate_init` controls the step size of our gradient descent, tuning this will affect the convergece speed and the ability of the model to escape local minimas.\n",
    "\n",
    "The `max_iter` parameter controls how many iterations of the solver for convergence. Tuning this parameter ensure our model converges. \n",
    "\n",
    "The `batch_size` parameter controls how many samples are processed before the model is updated. Tuning this parameter helps with model stability and speed for convergence. \n",
    "\n",
    "Finally, `solver` is the optimization algorithm used for training the network. `adam`: Adaptive optimization algorithm, efficient for larger datasets. `sgd`: Stochastic Gradient Descent, sensitive to learning rate and batch size. Finally, `lbfgs`: Limited-memory BFGS, a quasi-Newton method for smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'hidden_layer_sizes': [(8,), (16,), (32,), (16, 16), (32, 16)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'alpha': np.logspace(-4,-2, 5),\n",
    "    'learning_rate_init': np.logspace(-3.5, -1.5, 5),\n",
    "    'max_iter': [500, 1000, 1500, 2000],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'solver': ['adam', 'sgd', 'lbfgs'],\n",
    "}\n",
    "random_search_nn = RandomizedSearchCV(MLPClassifier(random_state=42), param_distributions=params, n_iter=100, cv=10, random_state=42, n_jobs=-1, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_nn.fit(X_train_scaled_nn, y_train_nn)\n",
    "print(f\"Best Parameters random search nn: {random_search_nn.best_params_}\")\n",
    "print(f\"Best Score random search nn: {random_search_nn.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn_tuned = random_search_nn.best_estimator_\n",
    "model_nn_tuned.fit(X_train_scaled_nn, y_train_nn)\n",
    "score_nn_tuned = model_nn_tuned.score(X_test_scaled_nn, y_test_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model test score and CV score\n",
    "best_cv_score = random_search_nn.best_score_\n",
    "test_score = model_nn_tuned.score(X_test_scaled_nn, y_test_nn)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['CV Accuracy', 'Test Accuracy'], [best_cv_score, test_score], color=['blue', 'green'])\n",
    "plt.title(\"Performance of the Best Neural Network Model\", fontsize=14)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Extract results into a DataFrame\n",
    "results_df = pd.DataFrame(random_search_nn.cv_results_)\n",
    "heatmap_data = results_df.pivot_table(\n",
    "    index='param_hidden_layer_sizes',\n",
    "    columns='param_alpha',\n",
    "    values='mean_test_score'\n",
    ")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap='coolwarm', cbar_kws={'label': 'Mean CV Accuracy'})\n",
    "plt.title(\"Hyperparameter Impact: Hidden Layer Size vs Alpha\", fontsize=14)\n",
    "plt.xlabel(\"Alpha (Regularization Strength)\", fontsize=12)\n",
    "plt.ylabel(\"Hidden Layer Sizes\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weights of the first hidden layer\n",
    "weights = model_nn_tuned.coefs_[0]  # Weights for the first layer\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(weights, cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title(\"Visualization of Weights (First Hidden Layer)\", fontsize=14)\n",
    "plt.xlabel(\"Neurons in Next Layer\", fontsize=12)\n",
    "plt.ylabel(\"Input Features\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model_nn_tuned.loss_curve_, marker='o')\n",
    "plt.title(\"Loss Curve During Training\", fontsize=14)\n",
    "plt.xlabel(\"Iterations\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = random_search_nn.cv_results_\n",
    "learning_rates = params['learning_rate_init']\n",
    "mean_test_scores_lr = [\n",
    "    np.mean([results['mean_test_score'][i]\n",
    "             for i in range(len(results['mean_test_score']))\n",
    "             if results['param_learning_rate_init'][i] == lr])\n",
    "    for lr in learning_rates\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(learning_rates, mean_test_scores_lr, marker='o')\n",
    "plt.axvline(random_search_nn.best_params_['learning_rate_init'], color='red', linestyle='--')\n",
    "plt.title(\"Effect of Learning Rate on Accuracy\", fontsize=14)\n",
    "plt.xlabel(\"Learning Rate (learning_rate_init)\", fontsize=12)\n",
    "plt.ylabel(\"Mean CV Accuracy\", fontsize=12)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report for tuned neural network:\")\n",
    "print(f\"Test Accuracy: {score_nn_tuned * 100:.2f}%\")\n",
    "print(classification_report(y_test_nn, model_nn_tuned.predict(X_test_scaled_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_test = model_nn_scaled.predict(X_test_scaled_nn)\n",
    "print(\"Confusion Matrix for scaled neural network without tuning:\")\n",
    "print(classification_report(y_test_nn, y_pred_test_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9. Model performance summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                   | Initial Model Configuration | Modified Model Configuration | Initial Accuracy | Final Accuracy |\n",
    "|-------------------------|-----------------------------|---------------------|-------------------|----------------|\n",
    "| kNN                     | k = 3                        | Best k = 8                   | 85.20%                 | 86.8%              \n",
    "| Logistic Regression     | default                   | -                 | 84.00%              | 85.40%\n",
    "| Neural Networks            | default                     |                    | n%                 | n%              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10. Insights and conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11. References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
